{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 09 mar 2018\n",
    "\n",
    "@author: Umberto\n",
    "'''\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from numpy import mean\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, GRU, Dropout\n",
    "from keras.layers import Bidirectional, Embedding, SpatialDropout1D, concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import nltk as nlp\n",
    "import gensim.models as gsm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from difflib import SequenceMatcher\n",
    "from nltk.corpus import words as nltk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch + 1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    \"\"\"\n",
    "    Use a few regular expressions to clean up pour data.\n",
    "    \"\"\"\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(s1, s2):\n",
    "    \"\"\"\n",
    "    Find Similarity between two strings.\n",
    "    Return a measure of the sequences' similarity (float in [0,1]).\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, s1, s2).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_word(dictionary, word):\n",
    "    \"\"\"\n",
    "    Check if a word is an English word.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = dictionary[word]\n",
    "        return True\n",
    "    except KeyError:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bad_word(word_list, dictionary, bad_words, threshold):\n",
    "    \"\"\"\n",
    "    Return a list of normalized words.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "\n",
    "    for word in word_list:\n",
    "        found = False\n",
    "        normalizedBadWord = \"\"\n",
    "        for badword in bad_words:\n",
    "            #if(similarity(badword, word) > threshold):\n",
    "            if(badword in word):\n",
    "                found = True\n",
    "                normalizedBadWord = badword\n",
    "                break;                \n",
    "        if(found):\n",
    "            res.append(normalizedBadWord)      \n",
    "        else:\n",
    "            if(is_english_word(dictionary, word)):\n",
    "                res.append(word)        \n",
    "    #print(res) \n",
    "    return res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(maxlen, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, units=0, dr=0.0):\n",
    "        inp = Input(shape=(maxlen,)) \n",
    "        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout1D\n",
    "        x = SpatialDropout1D(0.2)(x)\n",
    "        # https://stackoverflow.com/questions/43035827/whats-the-difference-between-a-bidirectional-lstm-and-an-lstm\n",
    "        x = Bidirectional(GRU(units, return_sequences=True, dropout=dr, recurrent_dropout=dr))(x)\n",
    "        # http://konukoii.com/blog/2018/02/19/twitter-sentiment-analysis-using-combined-lstm-cnn-models/\n",
    "        # For text, CNN -> LSTM (or GRU) doesn't seem to work well, but LSTM -> CNN works really well.\n",
    "        x = Conv1D(filters=64, kernel_size=2, padding='valid', kernel_initializer=\"he_uniform\")(x)\n",
    "        x = Dropout(dr)(x)\n",
    "        # x = MaxPooling1D(pool_size=2)(x)\n",
    "        # Global average pooling operation for temporal data.\n",
    "        # https://www.quora.com/What-is-global-average-pooling\n",
    "        avg_pool = GlobalAveragePooling1D()(x)\n",
    "        # Global max pooling operation for temporal data.\n",
    "        max_pool = GlobalMaxPooling1D()(x)\n",
    "        conc = concatenate([avg_pool, max_pool])\n",
    "        outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "        \n",
    "        model = Model(inputs=inp, outputs=outp)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(lr=lr, decay=lr_d),\n",
    "                      metrics=['accuracy']) \n",
    "    \n",
    "        print(model.summary())\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num bad words:  34\n",
      "num train:  159571\n",
      "num test:  153164\n",
      "Preparing Dictionary...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Â·'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-15f34ea11d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Preparing Dictionary...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Read the FastText word vectors (space delimited strings) into a dictionary from word->vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_coefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embeddings_index size: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-15f34ea11d1d>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Preparing Dictionary...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Read the FastText word vectors (space delimited strings) into a dictionary from word->vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_coefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embeddings_index size: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4a3efa694941>\u001b[0m in \u001b[0;36mget_coefs\u001b[0;34m(word, *arr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_coefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Â·'"
     ]
    }
   ],
   "source": [
    "#nlp.download()    \n",
    "\n",
    "bad_words = ['sex', 'suck', 'anal', 'penis', 'shit', 'fuck', 'damn', 'bitch', 'crap', 'piss', 'dick', 'darn', 'cock', 'pussy', 'ass', 'asshole', 'fag', 'bastard', 'slut', 'douche', 'bastard', 'darn', 'bloody', 'bugger', 'bollocks', 'arsehole', 'nigger', 'nigga', 'moron', 'gay', 'antisemitism', 'anti', 'nazi', 'poop']\n",
    "print(\"num bad words: \", len(bad_words))\n",
    "\n",
    "base_path_input = '../data/'\n",
    "base_path_output = ''\n",
    "# define path to save model\n",
    "model_path = base_path_output + 'keras_model.h5'\n",
    "\n",
    "EMBEDDING_FILE = '../FastText/wiki.en.vec' # 'fasttext-crawl-300d-2m/crawl-300d-2M.vec'   \n",
    "train = pd.read_csv(base_path_input + 'train.csv')\n",
    "test = pd.read_csv(base_path_input + 'test.csv') \n",
    "\n",
    "print(\"num train: \", train.shape[0])\n",
    "print(\"num test: \", test.shape[0])     \n",
    "\n",
    "embed_size = 300  # how big is each word vector\n",
    "max_features = 100000  # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 150  # max number of words in a comment to use\n",
    "\n",
    "stop_words = set(stopwords.words('english'))     \n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')   \n",
    "\n",
    "print('Preparing Dictionary...')\n",
    "# Read the FastText word vectors (space delimited strings) into a dictionary from word->vector\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE, encoding=\"utf8\"))    \n",
    "print(\"embeddings_index size: \", len(embeddings_index))\n",
    "dictionary = dict.fromkeys(embeddings_index, None)\n",
    "print(\"Dictionary size: \", len(dictionary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# DATA PREPARATION\n",
    "####################################################  \n",
    "\n",
    "# TRAIN\n",
    "train[\"comment_text\"].fillna('_NA_')\n",
    "train = standardize_text(train, \"comment_text\")\n",
    "train[\"tokens\"] = train[\"comment_text\"].apply(tokenizer.tokenize)\n",
    "# Delete Stop Words\n",
    "train[\"tokens\"] = train[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words])\n",
    "# Normalize Bad Words    \n",
    "train[\"tokens\"] = train[\"tokens\"].apply(lambda vec: normalize_bad_word(vec, dictionary, bad_words, 0.5))\n",
    "train.to_csv(base_path_output + 'train_normalized.csv', index=False)\n",
    "\n",
    "all_training_words = [word for tokens in train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "print(\"Min sentence length is %s\" % min(training_sentence_lengths))\n",
    "print(\"Mean sentence length is %s\" % mean(training_sentence_lengths))\n",
    "\n",
    "train[\"tokens\"] = train[\"tokens\"].apply(lambda vec :' '.join(vec))\n",
    "print(\"num train: \", train.shape[0])\n",
    "print(train.head())\n",
    "\n",
    "# TEST    \n",
    "test[\"comment_text\"].fillna('_NA_')\n",
    "test = standardize_text(test, \"comment_text\")\n",
    "test[\"tokens\"] = test[\"comment_text\"].apply(tokenizer.tokenize)\n",
    "# Delete Stop Words\n",
    "test[\"tokens\"] = test[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words])\n",
    "# Normalize Bad Words\n",
    "test[\"tokens\"] = test[\"tokens\"].apply(lambda vec: normalize_bad_word(vec, dictionary, bad_words, 0.5))    \n",
    "test.to_csv(base_path_output + 'test_normalized.csv', index=False)\n",
    "\n",
    "all_test_words = [word for tokens in test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))\n",
    "print(\"Min sentence length is %s\" % min(test_sentence_lengths))\n",
    "print(\"Mean sentence length is %s\" % mean(test_sentence_lengths))\n",
    "\n",
    "test[\"tokens\"] = test[\"tokens\"].apply(lambda vec :' '.join(vec))\n",
    "print(\"num test: \", test.shape[0])\n",
    "print(test.head())\n",
    "\n",
    "# Turn each comment into a list of word indexes of equal length (with truncation or padding as needed)\n",
    "list_sentences_train = train[\"tokens\"].values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"tokens\"].values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train) + list(list_sentences_test))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "# BUILD EMBEDDING MATRIX    \n",
    "print('Preparing embedding matrix...')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"word_index size: \", len(word_index))   \n",
    "words_not_found = []\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():        \n",
    "    if i >= max_features: \n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "\n",
    "if(len(words_not_found)>0):        \n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n",
    "    df = pd.DataFrame(words_not_found)\n",
    "    df.to_csv(base_path_output + \"word_not_found.csv\", header=None, index=False)\n",
    "\n",
    "# pd.DataFrame(embedding_matrix).to_csv(base_path_output + \"embedding_matrix.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# MODEL\n",
    "####################################################  \n",
    "\n",
    "model = get_model(maxlen, nb_words, embed_size, embedding_matrix, lr=1e-3, lr_d=0, units=128, dr=0.5)\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(X_t, y, test_size=0.1, random_state=233)\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)    \n",
    "\n",
    "# prepare callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=5,\n",
    "        mode='max',\n",
    "        verbose=1),\n",
    "    ModelCheckpoint(\n",
    "        model_path,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=0),\n",
    "    RocAuc         \n",
    "]\n",
    "\n",
    "history = model.fit(X_tra, y_tra, batch_size=128, epochs=2, validation_data=(X_val, y_val),\n",
    "                 callbacks=callbacks, verbose=2, shuffle=True)\n",
    "\n",
    "# summarize history for Accuracy\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "fig_acc.savefig(base_path_output + \"model_accuracy.png\")\n",
    "\n",
    "# summarize history for loss\n",
    "fig_loss = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "fig_loss.savefig(base_path_output + \"model_loss.png\")\n",
    "\n",
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)\n",
    "y_test = estimator.predict([X_te], batch_size=1024, verbose=2)\n",
    "sample_submission = pd.read_csv(base_path_input + \"jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(base_path_output + 'submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
